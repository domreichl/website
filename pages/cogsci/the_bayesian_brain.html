<div class="blog-content">
    <h1>The Bayesian Brain:<br>An Introduction to Predictive Processing</h1>

    <h2>The greatest theory of all time?</h2>
    <p>The more I learn about the Bayesian brain, the more it seems to me that the theory of predictive processing is about as important for cognitive science as the theory of evolution is for biology or the theory of relativity for physics.</p>
    <p>That is quite an ambitious statement. If our brains really are Bayesian, which is to say that predictive processing is the fundamental principle of cognition, this would mean that all our sensing, feeling, thinking, and doing is a matter of making predictions.</p>
    <p>In this blog post, I won't discuss the ample evidence&nbsp;we have for this theory,<sup>[1]</sup> but limit myself to presenting the idea itself. Though it is probably not the greatest theory of all time, it does have the potential to become the greatest theory in the history of cognitive science.</p>

    <h2>What is predictive processing?</h2>
    <p>During every moment of your life, your brain gathers statistics to adapt its <strong>model of the world</strong>, and this model's job is to <strong>generate predictions</strong>. Your brain is a prediction machine.&nbsp;Just as the heart's main function is to pump blood through the body, so the brain's main function is to make predictions about the body. For example, your brain predicts incoming sensory data: what you're about to perceive from within (interoception) as from without (exteroception).</p>
    <p>We used to think that perception is a simple feed-forward process. Say, you open your eyes and sensory data travels from your retina into your brain where it is fed forward through multiple levels of cortical processing, ultimately leading to a motor reaction. If your brain is Bayesian, however, it doesn't process sensory data like that. Instead, it uses <strong>predictive processing</strong>, also known as predictive coding,<sup>[2]</sup> to <em>predict</em> what your eyes will see <em>before</em> you get the actual data from the retina.</p>
    <p>Your brain runs an internal model of the causal order of world that continually creates predictions about what you expect to perceive. These predictions are then matched with what you actually perceive, and the divergence between <em>predicted</em> sensory data and <em>actual</em> sensory data yields a <strong>prediction error</strong>. The better a prediction, the better the fit and the less prediction error propagates up the hierarchy. Wait, what's a hierarchy?</p>
    <p>Your internal model, also called <em>generative model</em> because it generates predictions, is structured as a <strong>bidirectional hierarchical cascade</strong>:</p>
    <ul>
        <li>The model is a <em>cascade</em> because it involves multiple levels of processing, multiple cortical areas in the brain.</li>
        <li>The model is <em>hierarchical</em> because it comprises higher and lower processing layers: lower levels process simple data (e.g., visual stimuli, affective signals, or proprioceptive data), higher level process categorizations (e.g., object recognition, emotion classification, action selection), and the highest levels process mental states (e.g., mental imagery, emotion experience, conscious goals, planning, reasoning).<sup>[3]</sup></li>
        <li>The model is <em>bidirectional</em> because signals continually propagate in both directions: predictions move downward to <a href="https://en.wikipedia.org/wiki/Pyramidal_cell" target="_blank">pyramidal neurons</a> of lower levels while prediction errors move upward to pyramidal neurons of higher levels.</li>
    </ul>
    <p>Each processing layer predicts what's happening at the layer below and receives an error signal from that lower layer—a dynamic process that may or may not loop all the way down to incoming sensory data. A model is successful when the predictions it generates at every layer of the hierarchical cascade are accurate, producing only minimal prediction errors.</p>
    <p>If your predictions don't fit the actual data, you get a high prediction error that updates your internal model—to reduce further discrepancies between expectation and evidence, between model and reality. Your brain dislikes unfulfilled expectations, so it structures its model of the world and motivates action in such a way that more of its predictions come truer. Here's a schematic depiction of a bidirectional hierarchical cascade to see how this all works:</p>
    <image src="https://dominicreichl.com/wp-content/uploads/2023/01/predictive-processing-1024x576.jpg" alt="bidirectional hierarchical cascade during predictive processing in a Bayesian brain"></image>
    <p>The black circles in the diagram (at the prediction error sources) depict something I haven't explained yet, namely <strong>expected precisions</strong>. Precisions determine the weight of prediction errors. If your brain expects that a certain prediction error won't be particularly reliable or important, it decreases its weight and thus the extent to which the error can update your internal model.</p>
    <p>For example, if it's dark and foggy outside, you can't expect the visual information you receive to be particularly reliable, so you add <em>that</em> expectation—in the form of expected precisions that modulate error signals—to your predictions in order to prevent your model from being unduly distorted by unreliable data.</p>
    <p>In other words, precisions are a measure of uncertainty that expresses your brain's <em>trust</em> in the expected sensory data and the ensuing the error signals. The brain wants prediction errors to have a high signal-to-noise ratio.</p>
    <p>Expected precisions are formally equivalent to inverse variance and functionally equivalent to&nbsp;<strong>attention</strong>. When you pay a lot of attention to an object, you become confident that the information you get from it is relatively accurate. Say, you've been mindfully looking at a Ferrari under good lighting conditions, so you're pretty confident that it is red. The more attentive you are, the more weight you put on potential error signals. By contrast, if you're not really focused, your brain turns down the "error volume," i.e., the impact a prediction error can have on your model.</p>

    <h2>What makes the brain Bayesian?</h2>
    <p>The basic idea of <strong>Bayesian probability</strong>&nbsp;is that you update your beliefs in the light of new evidence. For instance, if you see a dog-like creature running towards you, your belief that it will attack you may be 34%; if you recognize that it's an unmuzzled pit bull, your belief that you'll be attacked may rise to 78%; if it starts barking, 91%; if another barking dog passes you from behind, the probability of your attack-belief being true drops again. That's a very handwavy example, but it illustrates how your certainty of a belief may change as you gain new evidence.</p>
    <p><strong>Bayes' law&nbsp;</strong>is the mathematical formalization of that idea:&nbsp;<em>P(B|E)=P(E|B)*P(B)/P(E)</em>, which calculates&nbsp;the conditional probability of your belief <em>B</em> being true, given evidence <em>E</em>. This is what the brain <em>approximates</em><sup>[4]</sup> during predictive processing. In particular, your brain updates its statistical model of the world by integrating prediction errors in accordance with Bayes' theorem—hence the name <em>Bayesian brain</em>.<sup>[5]</sup></p>
    <p>With your model's prediction or <em>prior probability P(B)</em> and the lower-level data <em>E</em> within a broader hypothesis space <em>P(E)</em>, your brain learns about the <em>likelihood P(E|B)</em> of&nbsp;<em>E</em>, given hypothesis <em>B</em>. Applying Bayes' rule, this yields a <em>posterior probabilityP(B|E)</em>, which determines the prediction error.<sup>[6]</sup> The error signal, modulated by an expected precision (attention), then propagates back upward to the next higher level where it updates the model, correcting your brain's probabilistic map of reality, and on goes the hypothesis testing. Imagine, for instance, you want to cross a street:<sup>[7]</sup></p>
    <p>(I) Based on a vast set of hypotheses <em>B</em> about the current situation, your brain computes a hierarchical cascade of predictions <em>P(B)</em>, including</p>
    <ul>
        <li>priors about how shapes, colors, and noises will change (for low-level perceptual inference),</li>
        <li>priors about you moving your eyes, head, and legs (for low-level active inference),</li>
        <li>priors about vehicles in motion and changing traffic lights (for high-level perceptual inference).</li>
    </ul>
    <p>(II) Your brain receives sensory data <em>E</em> about what's actually happening on the street and in your body, including</p>
    <ul>
        <li>exteroceptive data representing shapes, colors, and noises,</li>
        <li>proprioceptive data representing eye, head, and leg movements,</li>
        <li>conceptual data representing vehicles and traffic lights, and</li>
        <li>agentic data representing your relation to the goal state.</li>
    </ul>
    <p>(III) Your model contextualizes that data to get <em>P(E)</em> and estimates the likelihoods <em>P(E|B)</em> of the data, given your hypotheses.</p>
    <p>(IV) Your brain implicitly applies Bayes' theorem to estimate the posterior probabilities <em>P(B|E)</em> and determines prediction errors at the relevant processing layers by</p>
    <ul>
        <li>matching posteriors and priors regarding shapes, colors, and noises,</li>
        <li>matching posteriors and priors regarding your eye, head, and leg movements,</li>
        <li>matching posteriors and priors regarding vehicles and traffic lights, and</li>
        <li>matching the posterior and the prior regarding your goal state.</li>
    </ul>
    <p>(V) The prediction errors, weighed by expected precisions, propagate up the hierarchical cascade, level by level, and alter the corresponding set of hypotheses, thus updating the model to reduce future prediction errors; or they are actively reduced through motor signals to the muscles that move your eyes, head, and legs (more about that second option in a second).</p>
    <p>(VI) After enough cycles of hypothesis testing and model updating during constant sensorimotor interaction with the world, you'll find yourself at the other side of the street, assuming all goes well. This complex predictive process can be described as your brain continually implementing Bayes' theorem.</p>

    <h2>What does this explain?</h2>
    <p>The scope of the Bayesian brain hypothesis is extremely ambitious.&nbsp;It's meant to be a unifying framework for <em>all</em> neural, cognitive, and psychological phenomena. If true, predictive processing explains, at a computational level,&nbsp;<em>everything</em> about the brain and mind—for reasons we shall see soon. But let's first take a look at how the framework of predictive processing applies to various cognitive phenomena.</p>
    <p><strong>Perception</strong>&nbsp;is the prediction of sensory inputs and the inferential<sup>[8]</sup> process of minimizing prediction error by changing the internal predictive model. This is <em>perceptual inference</em>: reducing prediction error by updating your model so that sensory inputs match with prior expectations. Through perception, you make your model more similar to the world.</p>
    <ul>
    <li><em>Imagination</em>&nbsp;may be a side-function of the brain's predictive machinery based on high-level predictions that don't travel all the way down the hierarchical cascade and aren't matched with current sensory data, for example, by radically lowering the precision weighting on low-level error signals.</li>
    <li><em>Delusions</em> and <em>hallucinations</em> may stem from alterations in the ability to integrate incoming data with perceptual predictions (cf. Griffin &amp; Fletcher 2017).</li>
    </ul>
    <p><strong>Action</strong> is proprioceptive<sup>[9]</sup> prediction and the inferential process of minimizing prediction error by changing sensory inputs. This is <em>active inference</em> (also called <em>predictive control</em>):&nbsp;reducing prediction error by moving the body so that sensory inputs match with prior expectations. Through motor action, you make the world conform to your model. As the predicted proprioceptive states are not yet actual, actions change the world to make them so (which means that all your actions are essentially self-fulfilling prophecies).</p>
    <ul>
    <li><em>Reflex arcs</em> operate at the lowest layer of the active inferential process and work to fulfill proprioceptive predictions until the expected sensory input is obtained. Unlike with perceptual inference, the model parameters are not updated, but kept stable.</li>
    <li><em>Intentional behavior</em>&nbsp;is a high-level prediction about an abstract future goal state. Once a concrete opportunity to act arises, it entrains multilevel cascades of lower-level predictions to change the world in a way that makes the high-level prediction come true (i.e., achieve the goal state), thereby reducing long-term prediction error. The achievement of conscious goals is an example of predictive processing operating at large spatiotemporal scales.</li>
    </ul>
    <p><strong>Emotion</strong> is interoceptive<sup>[10]</sup> prediction and the active inferential process of minimizing prediction error by triggering physiological changes (cf. Seth 2013). For details, read <a href="https://dominicreichl.com/cognitive-science/emotions-as-predictions/" target="_blank"><em>Emotions as Predictions</em></a>.</p>
    <ul>
    <li><em>Drive&nbsp;</em>is the active-inferential process of minimizing interoceptive prediction error through high-level action. For example, when the brain detects low blood sugar levels through interoceptive inference and the resulting prediction errors aren't resolved through autonomic control (e.g., metabolize body fat), you will feel driven to minimize error signals through voluntary action (e.g., eat sugary food).</li>
    <li><em>Mental disorder</em>&nbsp;may be the inability to reduce interoceptive prediction errors.</li>
    </ul>
    <p><strong>Attention</strong> is expected precision, which modulates the weight of prediction errors. <em>Shifts in attention</em> result from the hyperprior that the same sensorimotor hypotheses (lower-level priors) should not be retained for too long because the world is an ever-changing place that will kill you if you don't adapt (which you can't if you never shift your attention, if you always give the same weight to the errors of related predictions).</p>
    <p><strong>Learning</strong>&nbsp;is the updating of your internal model based on prediction errors so that your predictions gradually improve. The better your predictions about the causal, probabilistic structure of the world, the more effectively you can engage with it. This is why teenagers move more fluently through space than toddlers, why you're less clumsy at playing a sport or a musical instrument after having practiced the relevant motor skills, and why it's important to pursue truth. All these are types of increased effectiveness due to better predictions.</p>
    <p><strong>Memory</strong> consists of the learned parameters of the internal model, whereas its non-acquired parameters would be the innate knowledge evolution has genetically built into the nervous system. Both parts are important determinants of the brain's predictions.</p>
    <p><strong>Self-awareness</strong> is the inferential process of minimizing prediction error by changing your internal self-model, i.e., the model that generates predictions about what's most likely to be 'you' (cf. Braun et al. 2018).</p>
    <ul>
        <li><em>Agency</em>&nbsp;arises from a good enough fit between your self-model's predictions and exteroceptive&nbsp;input. If the prediction error is too high, you feel like you're not in control of your actions.</li> 
        <li><em>Ownership</em> arises from a good enough fit between your self-model's predictions and proprioceptive input. If the prediction error is too high, you might experience that one of your limbs doesn't belong to you, or you might have an out-of-body experience.</li>
    </ul>
    <p><strong>Belief</strong> is a hyperprior: a systemic prior with a high degree of abstraction, a high-level prediction that entails general knowledge about the world. Some examples:</p>
    <ul>
        <li><em>Physical beliefs</em>. You expect things to change over time, you expect heavy objects to fall fast, and you expect that you can't move left and right at the same time.</li>
        <li><em>Physiological beliefs</em>. You expect to see something when you open your eyes and you expect fire to hurt and burn your skin.</li>
        <li><em>Psychological beliefs.</em> You expect a great performance to make you feel proud and you expect to feel regret when you shy away from a challenge.</li>
        <li><em>Social beliefs</em>. You expect happy people to smile, offensive words to trigger a reaction, and power to corrupt.</li>
        <li><em>Cultural beliefs</em>. You expect cars to slow down as they approach stop signs, special offers to be highlighted in stores, and handshakes not to last for an hour.</li>
    </ul>
    <p>Notice how the Bayesian brain also manifests in our <strong>values</strong>:</p>
    <ul>
        <li>We value <em>truth</em> because accurate beliefs about the world allow us to make good predictions.</li>
        <li>We value <em>honesty</em> and <em>authenticity</em> because we know what we can expect from honest, authentic people, which improves our prediction-making in social contexts that involves them.</li>
        <li>We value <em>simplicity</em> because simple beliefs enable us to generate high-level predictions quickly.</li>
        <li>We value <em>wisdom</em> because reflected life experience equips us with relatively reliable hyperpriors that minimize long-term prediction error.</li>
    </ul>
    <p>At the same time, the predictive nature of human cognition explains why we tend to be bound to habits, feel drawn to <strong>comfort zones</strong>, and shy away from uncertainty. Beyond habits, comforts, and certainties, our priors and hyperpriors are less reliable, which leads us to make worse predictions and get higher prediction errors. But high prediction errors are precisely what the brain works so hard to prevent. The effort of radically updating our models to&nbsp;accommodate novel, unfamiliar situations can be so high that the brain simply decides to trigger feelings of fear, anxiety, or discomfort to motivate us to stick to what we can more reliably predict, i.e., our habitual behavior patterns in comfort zones of soothing regularity.</p>
    <p>If we want to leave or expand our comfort zones, we must convince our brains that the immediately resulting high prediction errors are worth it at a broader timescale. Accordingly, we can conceptualize <strong>willpower</strong> as a hyperprior involving the meta-prediction that, in certain contexts, higher prediction errors in the short term will lead to lower prediction errors in the long term. Self-control&nbsp;may thus minimize overall prediction error. But how can the average prediction error over time even matter to a brain that constantly has to deal with immediate errors in any given moment? The answer can found in...</p>

    <h2>The free energy principle</h2>
    <p>Due to the second law of thermodynamics, the majority of your body's possible states are death or dysfunction. You're lucky that you're alive and able to read this. Sooner or later, <strong>entropy</strong> will catch up with each of us, and we will enter the physiologically unexpected yet physically probable and inevitable state of death. Naturally, this is precisely what we evolved to avoid. Organisms evolve by struggling for survival, fighting chaos, and resisting entropy while favoring states that are easily predictable.</p>
    <p>According to the <strong>free energy principle</strong>, we maintain homeostasis by suppressing our <em>free energy</em>, the information-theoretic equivalent of overall prediction error, averaged over the long term. Everything we do (and everything any living creature does) is, on average and over time, done to minimize free energy, which corresponds to the brain's job of minimizing prediction error.</p>
    <p>And all this to stay within expected, relatively stable states...—why? Because we can survive only within a certain range of physiological states. Too hot and we die, too cold and we die, too much oxygen and we die, too little oxygen and we die, et cetera. Too much or too little of anything will kill us, hence the fundamental physiological principle of <strong>homeostasis</strong>.</p>
    <p>To make a long story short, the following principles are hypothesized to be equivalent, though operating at different levels of description:</p>
    <ul>
        <li>cognitive level: minimize prediction error and maximize model fit,</li>
        <li>information-theoretic level: suppress free energy<sup>[11]</sup> and reduce surprisal,<sup>[12]</sup></li>
        <li>physiological level: maintain homeostasis,</li>
        <li>physical level: resist entropy,</li>
        <li>biological level: survive.</li>
    </ul>
    <p>Predictive processing captures what the brain contributes to the body's evolutionary fitness.<sup>[13]</sup> If we believe Friston (2009), the free energy principle can explain all structural and functional aspects of the brain, including its anatomy, connectivity, synaptic physiology, electrophysiology, and psychophysiology. I must concede, however, that there are many questions about free energy I have not found an answer to yet, and <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" target="_blank">a lot of math</a> that would go too far beyond the constraints of this article.</p>

    <h2>Five objections</h2>

    <h3>Sensory deprivation</h3>

    <p><strong>Problem.</strong> If all our brains want is low prediction errors, shouldn't the optimal strategy be to motivate us to lock ourselves up in a dark, silent room? What better way to make good predictions about expected sensory input than depriving the body of sensory stimulation? In total darkness, we can't get high prediction errors for expecting to see black, black, and black.</p>

    <p><strong>Answer.</strong> Even though this strategy may be effective (and relaxing) in the short term, the demands of the world and our bodies cannot be avoided for long. The brain, being a bodily organ, never ceases to generate interoceptive prediction cascades, no matter the degree of exteroceptive deprivation. Soon we'll feel bored, hungry, and driven to leave the room again, which will put our bodies in increasingly unexpected states.&nbsp;Locked in a dark, silent room for a long period, we can't maintain homeostasis. In Bayesian terms, we have a hyperprior that occupying the same state for too long will increase long-term prediction error. To always stay within a range of expected states, it's often more efficient to engage with a stimulus-rich environment (except, of course, when we need to sleep).</p>
    <h3>Human rationality</h3>

    <p><strong>Problem.</strong> Humans are notoriously bad at probabilistic reasoning, at thinking rationally about probabilities<sup>[14]</sup> So if mental processes can barely handle Bayesian inference, how can we assume that the brain, which produces the mind, is Bayesian?</p>

    <p><strong>Answer.</strong> The Bayesian brain is not inhabited by some homunculus doing complicated math. Its probabilistic inferences are not explicit, but implicit. Brain cells&nbsp;fire in a way that "naturally" approximates Bayes' law, just like ants move in a way that&nbsp;"naturally"&nbsp;approximates Gaussian and Pareto distributions&nbsp;without there being the need for a chief ant strategist who calculates the optimal path for his colony (cf. Vela-Pérez et al. 2015). What does "naturally" mean here? In the case of ants: pheromone secretion; in the case of neurons: Bayesian sampling.</p>

    <p>The Bayesian brain is a sampler, not a calculator: it samples information from a local landscape of probabilities (its environment). Over time, action and perception, associated with a practically infinite number of predictions, yield an uncountable number of samples from the unimaginably complex probability distributions that constitute reality. This enormous amount of data makes the brain asymptotically conform to the laws of probability, thus mechanistically realizing Bayes' theorem. Of course, the brain doesn't store all those samples; it just uses them to update its internal model of the world before it draws new samples from its current local environment.</p>

    <p>The human mind, by contrast, having only a finite number of samples, must rely on cognitive shortcuts (heuristics) to generate predictions. Although this has proven evolutionarily adaptive, it tends to make our conscious selves cognitively biased and statistically irrational. Sanborn &amp; Chater (2016) explain how sampling produces various reasoning biases. Remember, however, that laboratory studies on cognitive biases are conducted in extremely scarce environments. When people make everyday cognitive judgments in rich, familiar, realistic contexts, their reasoning can be much closer to Bayes optimality than with random judgments in a lab context where sampling is radically limited (cf.&nbsp;Griffiths &amp; Tenenbaum 2006 and Maguire et al. 2018).</p>

    <p>Moreover, consider how the <em>confirmation bias</em>&nbsp;(our tendency to focus on information that confirms our preconceptions) follows directly from the Bayesian brain hypothesis. After all, preconceptions are vital for the internal model our brain uses to make predictions. What we notice and focus on is highly determined by our preconceptions, by what we already know. And since predictive processing makes us search for whatever is most likely to verify our predictions, it will generally prefer confirmatory over surprising evidence as a strategy to minimize prediction error.</p>

    <h3>Offline cognition</h3>

    <p><strong>Problem.</strong> The brain does so much more than perceive the world and command motor actions. Not everything we do is directly related to perception and action. We plan holidays, make financial decisions, reflect on philosophical arguments, and ruminate about events that happened years ago. Such cognitive activities happen "offline," i.e., without immediate real-world interaction. How can predictive processing account for that?</p>

    <p><strong>Answer.</strong> Offline cognition is a form of high-level prediction-making that doesn't necessarily propagate predictions all the way down to the lowest layers of the hierarchical cascade—although it might at some point: then the timescales are simply much longer than those of "online" cognition, which is concerned mostly with immediate sensorimotor interactions in the present environment.</p>

    <h3>Phenomenology</h3>
    <p><strong>Problem.</strong> The world doesn't look like a complex set of intertwined probability density distributions. It looks rich and colorful and usually unambiguous, often beautiful. So how can we say that cognition is nothing but predictive processing? Is this not overly reductionistic?</p>
    <p><strong>Answer.</strong> Predictive processing is a theory about how the brain encodes information about the world, not how people experience it. Phenomenology operates at a higher level of description than cognitive science. Reversely, on an even lower level we could have physics describe cognitive processes in terms of atoms and subatomic particles, but likely to little avail. In any case, moving to a different level of description does not imply reductionism. Still, it would be interesting to investigate how much of psychology could be reduced to predictive processing.</p>

    <h3>Falsifiability</h3>
    <p><strong>Problem.</strong>&nbsp;If predictive processing can explain everything about neural and cognitive processes, does this not expose its triviality? Even more, researchers might retroactively tweak their prior assumptions to make any experimental result fit a Bayesian interpretation <em>post hoc</em>. This would mean&nbsp;that the theory effectively explains <em>nothing</em>. What keeps the Bayesian brain hypothesis from being unfalsifiable?</p>
    <p><strong>Answer.</strong> Evolution, too, is an extremely ambitious theory that aims to explain everything about life and biological systems with only a few basic tools. This famously invites a lot of post-hoc theorizing, especially in the field of evolutionary psychology where just-so stories are often isolated from alternative interpretations. But this doesn't automatically invalidate the theory. All it means is that we must think hard to come up with viable, testable alternative hypotheses and stipulate priors based on independent evidence. "For example," writes Hohwy (2015, p. 14), "there is independent evidence that we expect light to come more or less from above [...], that objects move fairly slowly [...], and that we expect others to look at us." Such kinds of evidence can be integrated with neuroscientific studies.</p>
    <p>Predictive processing and the free energy principle can be falsified. If descending prediction signals were found not to carry expected precisions, this would falsify the theory. Or it might turn out that some brain areas are not best described in terms of prediction error signaling. Lastly, the discovery of an organism that doesn't seek to maintain homeostasis by acting to stay within expected states would be a&nbsp;wholesale falsification, albeit a highly unlikely one.</p>

    <h2>Conclusion</h2>
    <p>Predictive processing provides a framework for understanding neuroscience and cognitive science at a computational level. Although the Bayesian brain theory is still in its fledgling stage, confirmatory evidence is flowing in on an almost weekly basis from a vast range of different fields. And although it offers a highly integrative and ambitious account of the brain and human cognition, it does leave much unspecified. For specification, we must also engage with the paradigms of evolution (because cognition has an evolutionary history), embodied embeddedness (because cognition operates in physical environments), and sociocultural situatedness (because cognition is situated in social and cultural contexts). Only if we cover and combine all levels of analysis can we truly understand how humans work.</p>

    <h2>References</h2>
    <p>Bowers JS, Davis CJ (2012).&nbsp;<a href="https://doi.org/10.1037/a0026450" target="_blank">Bayesian just-so stories in psychology and neuroscience</a>,&nbsp;<em>Psychological Bulletin</em>,&nbsp;138(3): 389-414.</p>
    <p>Braun N, Debener S, Spychala N, Bongartz E, Sörös P, Müller HHO, Philipsen A (2018).&nbsp;<a href="https://dx.doi.org/10.3389%2Ffpsyg.2018.00535" target="_blank">The Senses of Agency and Ownership: A Review</a>, <em>Frontiers in Psychology</em>, Vol. 9(535).</p>
    <p>Buckley CL, Kim CS, McGregor S, Seth AK (2017).&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0022249617300962" target="_blank">The free energy principle for action and perception: A mathematical review</a>, <em>Journal of Mathematical Psychology</em>, Vol. 81, pp. 55-79.</p>
    <p>Clark A (2015). <a href="https://www.amazon.com/Surfing-Uncertainty-Prediction-Action-Embodied/dp/0190217014" target="_blank">Surfing Uncertainty</a>, <em>Oxford University Press</em>.</p>
    <p>Clark A (2013). <a href="https://doi.org/10.1017/S0140525X12000477" target="_blank">Whatever next? Predictive brains, situated agents, and the future of cognitive science</a>, <em>Behavioral and Brain Sciences</em>, Vol.&nbsp;36(3), pp. 181-204.</p>
    <p>Friston KJ, Stephan Ke (2007).&nbsp;<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2660582/" target="_blank">Free-energy and the brain</a>, <em>Synthese</em>, Vol.&nbsp;159(3), pp. 417-458.</p>
    <p>Friston KJ (2009).&nbsp;<a href="https://doi.org/10.1016/j.tics.2009.04.005" target="_blank">The free-energy principle: a rough guide to the brain?</a>, <em>Trends in Cognitive Sciences</em>, Vol.&nbsp;13(7), pp. 293-301.</p>
    <p>Griffin JD, Fletcher PC (2017).&nbsp;<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5424073/" target="_blank">Predictive Processing, Source Monitoring, and Psychosis</a>,&nbsp;<em>Annual Review of Clinical Psychology</em>, Vol. 13, pp.&nbsp;265-289.</p>
    <p>Griffiths TL, Tenenbaum JB (2006).&nbsp;<a href="https://doi.org/10.1111/j.1467-9280.2006.01780.x" target="_blank">Optimal predictions in everyday cognition</a>, <em>Psychological Science</em>, Vol. 17(9), pp. 767-773.</p>
    <p>Harkness DL &amp; Keshava A (2017).&nbsp;<a href="https://predictive-mind.net/papers/moving-from-the-what-to-the-how-and-where-bayesian-models-and-predictive-processing" target="_blank">Moving from the What to the How and Where – Bayesian Models and Predictive Processing</a>,&nbsp;<em>Philosophy and Predictive Processing</em>, 16.</p>
    <p>Hohwy J (2015). <a href="https://open-mind.net/papers/the-neural-organ-explains-the-mind" target="_blank">The Neural Organ Explains the Mind</a>,&nbsp;<em>Open MIND</em>&nbsp;by Metzinger T &amp; Windt JM (Eds.), 19(T).</p>
    <p>Maguire P, Moser P, Maguire R, Keane MT (2018).&nbsp;<a href="https://doi.org/10.3389/fpsyg.2018.01011" target="_blank">Why the Conjunction Effect Is Rarely a Fallacy: How Learning Influences Uncertainty and the Conjunction Rule</a>, <em>Frontiers in Psychology</em>, Vol. 9(1011).</p>
    <p>Metzinger T, Wiese W (2017). <a href="https://predictive-mind.net/papers/vanilla-pp-for-philosophers-a-primer-on-predictive-processing" target="_blank">Vanilla PP for Philosophers: A Primer on Predictive Processing</a>, <em>Philosophy and Predictive Processing</em>, 1.</p>
    <p>Moseley JB, O'Malley K, Petersen NJ, Menke TJ, Brody BA, Kuykendall DH, Hollingsworth JC, Ashton CM, Wray NP (2002).&nbsp;<a href="https://doi.org/10.1056/NEJMoa013259" target="_blank">A controlled trial of arthroscopic surgery for osteoarthritis of the knee</a>,&nbsp;<em>The New England Journal of Medicine</em>, 347(2):81-88.</p>
    <p>Sanborn AN, Chater N (2016). <a href="https://doi.org/10.1016/j.tics.2016.10.003" target="_blank">Bayesian Brains without Probabilities</a>, <em>Trends in Cognitive Science</em>, Vol. 20(12), pp.&nbsp;883-893.</p>
    <p>Seth AK (2013).&nbsp;<a href="https://doi.org/10.1016/j.tics.2013.09.007" target="_blank">Interoceptive inference, emotion, and the embodied self</a>, <em>Trends in Cognitive Science</em>, Vol. 17(11), pp. 565-573.</p>
    <p>Thornton C (2016). <a href="https://doi.org/10.1016/j.bandc.2016.03.004" target="_blank">Predictive processing simplified: The infotropic machine</a>, <em>Brain and Cognition</em>, Vol. 112, pp. 13-24.</p>
    <p>Vela-Pérez M, Fontelos MA, Garnier S (2015). <a href="https://doi.org/10.1016/j.mbs.2015.01.007" target="_blank">From individual to collective dynamics in Argentine ants (Linepithema humile)</a>, <em>Mathematical Biosciences</em>, Vol. 262, pp. 55-64.</p>

    <h2>Footnotes</h2>
    <p>[1] Some noteworthy evidence includes <a href="https://www.nature.com/articles/nature03689" target="_blank">predictive coding by the retina</a>, <a href="https://doi.org/10.1016/j.cognition.2008.05.010" target="_blank">binocular rivalry</a>, <a href="https://dx.doi.org/10.1371%2Fjournal.pone.0151194" target="_blank">perceptual completion</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2747248/" target="_blank">repetition suppression</a>,&nbsp;<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4374459/" target="_blank">multisensory integration</a>, <a href="https://doi.org/10.1007/s00429-014-0942-2" target="_blank">motion perception</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4037840/" target="_blank">synesthesia</a>, and&nbsp;<a href="https://www.nature.com/articles/nn0199_79" target="_blank">non-classical receptive-field effects</a>.</p>
    <p>[2] <em>Predictive coding</em> is a data compression strategy used, e.g., for the lossless compression of digital media files, whereas <em>predictive processing</em> is the use of that strategy to explain how hierarchical generative models flexibly combine up- and downward flows of information in the nervous system to minimize precision-weighted prediction errors. In short, predictive processing is the application of predictive coding in a cognitive context; even shorter, predictive processing is hierarchical predictive coding.</p>
    <p>[3] In general, lower levels, also called <em>unconscious</em> or <em>subpersonal </em>levels, are more automatic, concrete, detailed, spatially and temporally precise, and associated with specific sensory or motor areas, whereas higher levels, with the <em>agentic</em>&nbsp;or&nbsp;<em>personal </em>level on top, are more voluntary, abstract, complex, integrative, and distributed over multiple cortical areas, especially associative ones. A clear example of that hierarchy comes from language processing where we have first, on the lowest level, an auditory stimulus, then phonetics, then phonology, then morphology, then syntax, then semantics, then a situation model, and finally the communicated message.</p>
    <p>[4] Your internal model only ever approximates Bayes optimality because there is no homunculus sitting in your brain doing math and because your neural resources are limited: your brain has to deal with limited memory, limited attentional capacity, limited space, limited time, and incomplete information.</p>
    <p>[5] I should mention here that, according to Thornton (2017), predictive processing could also be specified by&nbsp;<a href="https://en.wikipedia.org/wiki/Information_theory" target="_blank">Shannon information theory</a>&nbsp;as an alternative to Bayesian inference.</p>
    <p>[6] Formally, the prediction error is calculated as the <a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence" target="_blank">KL divergence</a> between prior probabilities P(B) and posterior probabilities P(B|E); cf. Jakob Hohwy (2014), <em>The Predictive Mind</em>, p. 58.</p>
    <p>[7] I'm again very casual in my description here; for a scientific example, cf.&nbsp;Harkness &amp; Keshava (2017).</p>
    <p>[8] Perception is&nbsp;<em>inferential</em> because it <em>infers</em> the latent causes of observations. Predictions are <em>inferences</em> about the causal structure of the world.</p>
    <p>[9] <em>Proprioception</em> is the ability to sense the relative position of your body parts through <del>signals from</del>predictions about the dynamic states of muscle spindles, tendons, and joints.</p>
    <p>[10] <em>Interoception</em> is the ability to sense the internal condition of your body through <del>signals from</del>predictions about physiological states.</p>
    <p>[11] I'm putting free energy at the information-theoretic level because the <em>variational free energy</em> discussed here is not the same as the concept of free energy in thermodynamics (cf. Buckley et al. 2017 and Friston &amp; Stephan 2007).</p>
    <p>[12] <a href="https://en.wikipedia.org/wiki/Surprisal_analysis" target="_blank"><em>Surprisal</em></a> is a technical term for "surprise" that denotes how unlikely an event is, given a model. For example, your brain gets "surprised" by an unmet expectation (surprising sensory input) and a high prediction error. Since your brain hates nothing more than "surprise," it does everything it can to avoid it. Mathematically, surprisal is the negative logarithm of an event's probability.</p>
    <p>[13] At least in terms of an animal's survival; the survival of its genes through reproduction is yet another story.</p>
    <p>[14] Examples include numerous logical fallacies and cognitive biases such as&nbsp;<a href="https://en.wikipedia.org/wiki/Base_rate_fallacy" target="_blank">base rate neglect</a>, <a href="https://en.wikipedia.org/wiki/Neglect_of_probability" target="_blank">probability neglect</a>,&nbsp;<a href="https://en.wikipedia.org/wiki/Gambler%27s_fallacy" target="_blank">gambler's fallacy</a>, <a href="https://en.wikipedia.org/wiki/Prosecutor%27s_fallacy" target="_blank">prosecutor's fallacy</a>, <a href="https://en.wikipedia.org/wiki/Subadditivity_effect" target="_blank">subadditivity effect</a>, <a href="https://en.wikipedia.org/wiki/Conjunction_fallacy" target="_blank">conjunction fallacy</a> (but cf. Maguire et al. 2018), and many more.</p>
</div>
